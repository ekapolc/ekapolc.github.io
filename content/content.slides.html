<!-- RESEARCH -->
<div class="container-fluid">
  <div class="row">
    <div class="col-md-12" align="left">
      <h3>Slides from Public Talks</h3>
      <hr/>
      <h4>Deep Learning: Trends and Applications</h4>
      <h5>Given at Chulalongkorn University - July 14 2017</h5>
      <h5><a href="slides/DNN-trends-CP-7-14-17.pdf" target="_blank">Slides</a></h5>
      <h6>Deep learning has become the main vehicle for many machine learning applications. This talk aims to give the audience the general impression of what deep learning is and what can we use it for. We will briefly explain the difference between each kind of deep learning architectures and their proper usage. Then, we will discuss different applications for deep learning and trends in deep learning research.</h6>
      <h4>Deep Learning and Hardware: Matching the Demands from the Machine Learning Community</h4>
      <h5>Given at eHPC2018: Workshop on e-Science and High Performance Computing - July 12 2018</h5>
      <h5><a href="slides/eHPC_Deep_learning_and_hardware.pdf" target="_blank">Slides</a></h5>
      <h6>Deep learning or artificial neural networks have been gaining a lot of attention in the recent years, achieving state-of-the-art performance in many standard tasks such as automatic speech recognition, object recognition, and natural language processing. The formula for deep learning's success includes three key factors 1) the amount of data, 2) the computational capacity to handle the data, and 3) a large enough model capacity to handle the idiosyncrasy of the data. Larger and larger models with ever increasing amount of data have been developed. This significantly increases the amount of computation required for deep learning approaches. For example, AlphaGo Zero requires an estimate of 10^23 flops of computation to train. These kinds of compute requirement demands careful orchestration between software and hardware. In this talk, we will go over the techniques often employed in order to scaling up deep learning research. Additionally, we will also talk about an unconventional approach of using low-precision Logarithmic Number System (LNS) arithmetic for deep learning instead of the traditional float-based arithmetic which can greatly lower the hardware size and the power consumption per flop. With some modifications to the original back propagation algorithm, we were able to maintain the accuracy of traditional hardware with the LNS arithmetic.</h6>
    </div>
  </div><!-- /.row -->
</div>
